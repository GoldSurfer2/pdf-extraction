#!/usr/bin/env python3
"""
ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ëª¨ë‹¬ RAG í‰ê°€
Docling(í…ìŠ¤íŠ¸ ë³´ì¡´) + GPT-4o-mini(ì‹œê° ë¶„ì„) + ë‹µë³€ ìƒì„± LLM
"""

import json
import logging
import os
import re
import time
from typing import Dict, List, Any
from multimodal_rag import MultimodalRAG
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI()

def split_docling_text(text: str) -> List[Dict[str, Any]]:
    """LangChain RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ ê¸°ë°˜ Docling í…ìŠ¤íŠ¸ ë¶„í• """
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    docs = []
    
    # LangChain RecursiveCharacterTextSplitter ì´ˆê¸°í™”
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,           # ì²­í¬ í¬ê¸°
        chunk_overlap=200,         # ì²­í¬ê°„ ì˜¤ë²„ë©ìœ¼ë¡œ ë§¥ë½ ë³´ì¡´
        length_function=len,       # ê¸¸ì´ í•¨ìˆ˜
        separators=[              # ë¶„í•  ìš°ì„ ìˆœìœ„ (ì˜ë¯¸ì  ê²½ê³„ ìš°ì„ )
            "\n## ",              # ë§ˆí¬ë‹¤ìš´ í—¤ë” (ìµœìš°ì„ )
            "\n### ",             # ì„œë¸Œí—¤ë”
            "\n#### ",            # ë” ì‘ì€ í—¤ë”
            "\n\n",               # ë¬¸ë‹¨ êµ¬ë¶„
            "\n",                 # ì¤„ë°”ê¿ˆ
            ". ",                 # ë¬¸ì¥ ì¢…ë£Œ
            " ",                  # ë‹¨ì–´ êµ¬ë¶„
            ""                    # ë¬¸ì ë‹¨ìœ„ (ìµœí›„)
        ],
        is_separator_regex=False   # ì •ê·œì‹ ì‚¬ìš© ì•ˆí•¨
    )
    
    # í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤í–‰
    chunks = text_splitter.split_text(text)
    
    print(f"ğŸ”„ LangChain ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # ì²­í¬ í¬ê¸° í†µê³„
    chunk_lengths = [len(chunk) for chunk in chunks]
    avg_length = sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0
    min_length = min(chunk_lengths) if chunk_lengths else 0
    max_length = max(chunk_lengths) if chunk_lengths else 0
    
    print(f"ğŸ“Š ì²­í¬ í¬ê¸° í†µê³„:")
    print(f"   - í‰ê· : {avg_length:.0f}ì")
    print(f"   - ìµœì†Œ: {min_length}ì")
    print(f"   - ìµœëŒ€: {max_length}ì")
    
    # ê° ì²­í¬ë¥¼ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜
    for i, chunk in enumerate(chunks):
        docs.append({
            "page_content": chunk,
            "metadata": {
                "source": f"LangChain ì²­í¬ {i+1}",
                "chunk_id": i,
                "chunk_size": len(chunk),
                "type": "langchain_semantic_chunk"
            }
        })
    
    return docs

def generate_answer(query: str, retrieved_docs: List[str]) -> Dict[str, Any]:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì‹œê°„ ë° í† í° ì •ë³´ í¬í•¨)"""
    start_time = time.time()
    
    context = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{doc}" for i, doc in enumerate(retrieved_docs[:3])])
    
    system_message = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ê·œì¹™:
1. ë¬¸ì„œì— ëª…ì‹œëœ êµ¬ì²´ì  ì •ë³´(ìˆ«ì, ë‚ ì§œ, ì´ë¦„ ë“±)ë¥¼ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”  
3. ë‹µë³€ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ê°€ëŠ¥í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”"""

    user_message = f"""ì§ˆë¬¸: {query}

ì°¸ê³  ë¬¸ì„œë“¤:
{context}

ìœ„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.1,
            max_tokens=300
        )
        
        end_time = time.time()
        
        return {
            "answer": response.choices[0].message.content,
            "processing_time": end_time - start_time,
            "token_usage": response.usage.total_tokens,
            "input_tokens": response.usage.prompt_tokens,
            "output_tokens": response.usage.completion_tokens
        }
    except Exception as e:
        logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
        end_time = time.time()
        return {
            "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "processing_time": end_time - start_time,
            "token_usage": 0,
            "input_tokens": 0,
            "output_tokens": 0
        }

def main(input_file: str = "beverage_tech_result.json", custom_queries: List[str] = None):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ RAG í…ŒìŠ¤íŠ¸
    
    Args:
        input_file: í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
        custom_queries: ì‚¬ìš©ì ì •ì˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©)
    """
    try:
        # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ ë¡œë”©
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            hybrid_results = json.load(f)
        
        # 2. ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ìƒì„±
        multimodal_docs = []
        
        # Docling í…ìŠ¤íŠ¸ ë¬¸ì„œ (ì„¸ë°€í•œ ì„¹ì…˜ë³„ ë¶„í• )
        docling_text = hybrid_results["docling_extraction"]["markdown_text"]
        docling_docs = split_docling_text(docling_text)
        multimodal_docs.extend(docling_docs)
        
        # GPT-4o-mini í˜ì´ì§€ë³„ ì‹œê° ë¶„ì„
        gpt_analysis = hybrid_results.get("gpt4_visual_analysis", {})
        visual_analyses = gpt_analysis.get("visual_analyses", [])
        visual_count = 0
        
        for analysis_data in visual_analyses:
            page_num = analysis_data.get("page")
            visual_analysis = analysis_data.get("visual_analysis", "")
            has_visuals = analysis_data.get("has_visuals", False)
            
            if visual_analysis and visual_analysis.strip() and has_visuals:
                doc = {
                    "page_content": f"í˜ì´ì§€ {page_num} ì‹œê° ìš”ì†Œ ë¶„ì„:\n{visual_analysis}",
                    "metadata": {
                        "source": "GPT4o-mini_visual",
                        "page": page_num,
                        "type": "visual_analysis",
                        "has_visuals": has_visuals
                    }
                }
                multimodal_docs.append(doc)
                visual_count += 1
        
        print(f"ğŸ”¥ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ëª¨ë‹¬ RAG êµ¬ì¶•")
        print(f"ğŸ“Š ì´ ë¬¸ì„œ: {len(multimodal_docs)}ê°œ")
        print(f"ğŸ¯ GPT ì‹œê° ë¶„ì„: {visual_count}ê°œ í˜ì´ì§€") 
        print(f"ğŸ“ Docling í…ìŠ¤íŠ¸ ì„¹ì…˜: {len(docling_docs)}ê°œ")
        print(f"ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(docling_text):,}ì")
        
        # 4. RAG ì‹œìŠ¤í…œ ìƒì„± (GPU 0ë²ˆ ì‚¬ìš© - ë©”ëª¨ë¦¬ ì—¬ìœ  ì¶©ë¶„)
        import os
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # GPU 0ë²ˆ ì‚¬ìš©
        
        complete_rag = MultimodalRAG(
            text_model_name="intfloat/multilingual-e5-large",
            vision_model_name="openai/clip-vit-base-patch32",
            backend="faiss"
        )
        
        texts = [doc["page_content"] for doc in multimodal_docs]
        metadatas = [doc["metadata"] for doc in multimodal_docs]
        complete_rag.add_text_documents(texts, metadatas)
        
        # 5. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¤ì •
        if custom_queries:
            test_queries = custom_queries
            print(f"ğŸ¯ ì‚¬ìš©ì ì •ì˜ ì§ˆë¬¸ {len(test_queries)}ê°œ ì‚¬ìš©")
        else:
            # ê¸°ë³¸ ë°œí‘œìë£Œ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
            test_queries = [
                "Ball Rolling Pendulum ìŠ¤í…Œì´ì§€ì™€ Roller Rolling Pendulum ìŠ¤í…Œì´ì§€ì˜ êµ¬ì¡°ì  ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "íƒ„ì„± ë§ˆì°°ì²´ë¥¼ ì´ìš©í•œ ê°€ë³€ ì••ì°© ë§ˆì°° êµ¬ì¡°ì˜ ì‘ë™ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "Yaw ëª¨ì…˜ì„ ì¤„ì´ê¸° ìœ„í•´ ê³ ì•ˆëœ 'ëŒ€ì¹­ ë§ˆì°° êµ¬ì¡°'ëŠ” ê¸°ì¡´ì˜ 'ì„¼í„° ë§ˆì°° ë°©ì‹'ê³¼ ë¹„êµí•˜ì—¬ ì–´ë–¤ ì¥ì ì´ ìˆë‚˜ìš”?",
        "ì„œë¹™ ë¡œë´‡ì´ ê²½ì‚¬ë©´ì„ ì£¼í–‰í•  ë•Œ ìŠ¤í…Œì´ì§€ë¥¼ Lockingí•˜ëŠ” ê¸°ëŠ¥ì´ í•„ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ìŒì‹ì˜ ì¢…ë¥˜ì™€ ì–‘ì— ë”°ë¼ ê°ì‡ ë ¥ì„ ì¡°ì ˆí•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì´ë©°, ì†Œí˜• ìŒë£Œì™€ ì¤‘ëŒ€í˜• êµ­/íƒ•ì„ ë°°ë‹¬í•  ë•Œ ê°ì‡  ì¡°ì ˆì˜ ì´ˆì ì€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€ìš”?"
            ]
            print(f"ğŸ¯ ê¸°ë³¸ ì§ˆë¬¸ {len(test_queries)}ê°œ ì‚¬ìš©")
        
        print(f"\nğŸ§ª ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ëª¨ë‹¬ RAG í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
        total_time = 0
        total_tokens = 0
        total_input_tokens = 0
        total_output_tokens = 0
        
        for i, query in enumerate(test_queries, 1):
            try:
                # 1. ë¬¸ì„œ ê²€ìƒ‰
                results = complete_rag.search_text(query, n_results=5)
                docs = results.get('documents', [[]])[0]
                
                print(f"\nğŸ” ì§ˆë¬¸ {i}: {query}")
                print("-" * 50)
                
                if docs:
                    # 2. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì‹œê°„/í† í° ì¸¡ì •)
                    result = generate_answer(query, docs)
                    answer = result['answer']
                    processing_time = result['processing_time']
                    token_usage = result['token_usage']
                    input_tokens = result['input_tokens']
                    output_tokens = result['output_tokens']
                    
                    # ì „ì²´ ì§€í‘œì— ëˆ„ì 
                    total_time += processing_time
                    total_tokens += token_usage
                    total_input_tokens += input_tokens
                    total_output_tokens += output_tokens
                    
                    print(f"ğŸ’¡ ë‹µë³€: {answer}")
                    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
                    print(f"ğŸª™ í† í° ì‚¬ìš©: {token_usage}ê°œ (ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens})")
                    
                    # 3. ì°¸ê³ í•œ ë¬¸ì„œ ì •ë³´ì™€ ì¶œì²˜ í‘œì‹œ
                    metadatas = results.get('metadatas', [[]])[0]
                    distances = results.get('distances', [[]])[0]
                    
                    print(f"\nğŸ“„ ì°¸ê³ í•œ ìƒìœ„ ë¬¸ì„œ:")
                    used_docs = min(3, len(docs))  # ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œë§Œ í‘œì‹œ
                    
                    for j in range(used_docs):
                        doc = docs[j]
                        metadata = metadatas[j] if j < len(metadatas) else {}
                        distance = distances[j] if j < len(distances) else "N/A"
                        
                        # ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
                        source = metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                        doc_type = metadata.get('type', 'ì¼ë°˜')
                        
                        if source == 'Docling_section':
                            section = metadata.get('section', 'N/A')
                            location = f"Docling ì„¹ì…˜ {section}"
                        elif source == 'GPT4o-mini_visual':
                            page = metadata.get('page', 'N/A')
                            location = f"GPT í˜ì´ì§€ {page} ë¶„ì„"
                        else:
                            location = f"{source} ({doc_type})"
                        
                        preview = doc[:120].replace('\n', ' ').strip()
                        similarity = f"{(1-float(distance))*100:.1f}%" if distance != "N/A" else "N/A"
                        
                        print(f"   {j+1}. ğŸ“ {location} (ìœ ì‚¬ë„: {similarity})")
                        print(f"      ğŸ’¬ {preview}...")
                        print()
                else:
                    print("ğŸ’¡ ë‹µë³€: ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                print(f"âŒ ì§ˆë¬¸ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        print("\n" + "=" * 60)
        
        print(f"\nğŸ‰ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ëª¨ë‹¬ RAG ì™„ë£Œ!")
        print(f"âœ… ì„¸ë°€í•œ ë¬¸ì„œ ë¶„í•  + ê²€ìƒ‰ + LLM ë‹µë³€ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
        print(f"ğŸ“Š Docling ({len(docling_docs)}ê°œ ì„¹ì…˜) + GPT-4o-mini ({visual_count}ê°œ í˜ì´ì§€) í†µí•©")
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
        print("=" * 40)
        print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ (í‰ê· : {total_time/len(test_queries):.2f}ì´ˆ)")
        print(f"ğŸª™ ì´ í† í° ì‚¬ìš©: {total_tokens}ê°œ (í‰ê· : {total_tokens/len(test_queries):.0f}ê°œ)")
        print(f"   - ì…ë ¥ í† í°: {total_input_tokens}ê°œ")
        print(f"   - ì¶œë ¥ í† í°: {total_output_tokens}ê°œ")
        print(f"ğŸ¯ ì²˜ë¦¬ ì§ˆë¬¸ ìˆ˜: {len(test_queries)}ê°œ")
        print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${total_tokens * 0.000015:.4f} (GPT-4o-mini ê¸°ì¤€)")
        
    except Exception as e:
        logger.error(f"ì™„ì „í•œ ë©€í‹°ëª¨ë‹¬ RAG êµ¬ì¶• ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ëª¨ë‹¬ RAG í‰ê°€")
    parser.add_argument("--input", type=str, default="beverage_tech_result.json", 
                        help="í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--queries", nargs='+', default=None,
                        help="ì‚¬ìš©ì ì •ì˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)")
    
    args = parser.parse_args()
    
    main(input_file=args.input, custom_queries=args.queries)
